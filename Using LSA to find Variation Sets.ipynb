{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis\n",
    "\n",
    "Latent Semantic Analysis (LSA) was basically the top of the line as far as word vectors were concerned before the word2vec model [cite] was developed. The long and short of it is that you do a term/document matrix and do a Singular-Value Decomposition (SVD) [cite] on it. The result removes variation from the character representations of words. \n",
    "\n",
    "## Baseline\n",
    "\n",
    "Right now it doesn't make sense to compare it to anything with a concrete minimum matching. But if we hit an F1 score of 60, then I will consider this a viable option for detecting variation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utterances\n",
    "import evaluation\n",
    "import sys\n",
    "import difflib\n",
    "import collections\n",
    "import codecs\n",
    "from math import log\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import varseta_accuracy_tester as vat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = (\"anch\", 3, 2)\n",
    "\n",
    "to_dos = [\n",
    "        (\"DATA/Swedish_MINGLE_dataset/plain/1\", \"DATA/Swedish_MINGLE_dataset/GOLD/1\"),\n",
    "        (\"DATA/Swedish_MINGLE_dataset/plain/2\", \"DATA/Swedish_MINGLE_dataset/GOLD/2\"),\n",
    "        (\"DATA/Swedish_MINGLE_dataset/plain/3\", \"DATA/Swedish_MINGLE_dataset/GOLD/3\"),\n",
    "        (\"DATA/Swedish_MINGLE_dataset/plain/4\", \"DATA/Swedish_MINGLE_dataset/GOLD/4\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data\n",
    "\n",
    "Things to note:\n",
    "I'm using only the lowercase versions, considering the corpus size, this can change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in: DATA/Swedish_MINGLE_dataset/plain/1\n",
      "Reading in: DATA/Swedish_MINGLE_dataset/plain/2\n",
      "Reading in: DATA/Swedish_MINGLE_dataset/plain/3\n",
      "Reading in: DATA/Swedish_MINGLE_dataset/plain/4\n"
     ]
    }
   ],
   "source": [
    "all_utterances = []\n",
    "for to_do in to_dos:\n",
    "    print(\"Reading in: \" + to_do[0])\n",
    "    u = utterances.Utterances(to_do[0], to_do[1])\n",
    "    gold_utterances = u._goldutterances\n",
    "\n",
    "    utterances_reformatted = []\n",
    "    ids = []\n",
    "\n",
    "    for utterance in u._utterances:\n",
    "        new_utt = utterance[2].split()\n",
    "        new_utt = [i.lower() for i in new_utt]\n",
    "        utterances_reformatted.append(new_utt)\n",
    "        ids.append((utterance[0], utterance[1]))\n",
    "        \n",
    "    all_utterances = all_utterances + utterances_reformatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf\n",
    "\n",
    "The first step is to build a tf-idf matrix. For our purposes, each utterance will be a document. This may change down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dummy_preprocessor(to_return):\n",
    "    \"\"\"This is a workaround for the TfidfVectorizer's tokenizer\"\"\"\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(analyzer='word',\n",
    "                         tokenizer=_dummy_preprocessor,\n",
    "                         preprocessor=_dummy_preprocessor,\n",
    "                         token_pattern=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u's\\xe5', u'!']\n"
     ]
    }
   ],
   "source": [
    "tf_idf_features = tf_idf.fit_transform(all_utterances)\n",
    "tf_idf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the SVD\n",
    "\n",
    "Note, we can put all this in a pipeline, but I think it's a little more explecit if we just go through each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.51364892e-01, -2.31301547e-01,  1.16082492e-01, ...,\n",
       "         1.25349359e-03, -1.37946403e-03,  5.15523863e-04],\n",
       "       [ 1.46198434e-01,  2.32317490e-01,  7.58477293e-03, ...,\n",
       "         5.23653005e-03,  1.57735245e-03, -1.13509976e-03],\n",
       "       [ 4.49652796e-02,  7.92418302e-02,  9.63618728e-03, ...,\n",
       "         1.67115632e-03,  3.24197397e-02,  1.27179032e-02],\n",
       "       ...,\n",
       "       [ 3.59692288e-01,  1.90604597e-02, -8.16234413e-02, ...,\n",
       "        -1.79658585e-02, -1.33032014e-02, -5.86318483e-03],\n",
       "       [ 4.20428058e-02,  1.31244813e-01, -1.60543821e-01, ...,\n",
       "        -1.80693766e-03, -1.69740224e-01, -2.24321818e-02],\n",
       "       [ 5.82134770e-02,  2.08614156e-01, -2.59514807e-01, ...,\n",
       "         2.21563892e-02, -1.54063723e-01, -4.38398196e-03]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components=100, \n",
    "                   algorithm='randomized',\n",
    "                   n_iter=10, random_state=69)\n",
    "\n",
    "lsa.fit_transform(tf_idf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.29011349e-03  2.52727211e-03 -6.34374583e-04  5.55326881e-03\n",
      "  -2.39245892e-04 -1.48188405e-03 -8.16205523e-04  8.42298315e-04\n",
      "  -5.04169499e-03 -2.30353940e-06 -2.36536407e-03 -2.89080734e-03\n",
      "  -1.18691833e-03 -3.04637112e-03  2.09389575e-03 -2.18931325e-04\n",
      "  -3.96857868e-03  1.26094287e-02  6.75262106e-03  3.08682846e-03\n",
      "   3.75317972e-03 -2.40704326e-03 -1.56736199e-03  2.42793815e-03\n",
      "   8.20006228e-04 -1.04234446e-03  8.62725497e-03  2.07789017e-03\n",
      "   2.97357059e-03 -5.56152844e-03 -4.86702876e-03 -2.18053561e-03\n",
      "  -8.87283736e-04 -7.30840720e-04 -1.10542776e-03  2.74007910e-04\n",
      "  -4.39758640e-03 -2.51087103e-04 -1.23747322e-03 -1.52310575e-03\n",
      "  -4.99929199e-03 -7.37718676e-03 -1.84183931e-03  3.44890793e-03\n",
      "   3.63043851e-03 -2.83421161e-03 -2.31805620e-03 -2.04820630e-03\n",
      "   2.62852920e-02 -6.32734432e-03  8.95271579e-04  1.41285686e-03\n",
      "  -4.29371697e-03 -7.01372590e-04 -1.13206355e-03 -6.24236235e-03\n",
      "  -3.60416703e-03  5.59606565e-04 -7.31797130e-03  2.91480704e-03\n",
      "   1.49369202e-03 -4.20431171e-03 -1.59408992e-03  7.22910767e-03\n",
      "  -1.00392711e-02 -1.22113933e-02  1.39145174e-03  9.00902174e-03\n",
      "   2.84723931e-03 -7.53228808e-03 -4.94104253e-03 -3.97148266e-03\n",
      "  -9.34966202e-03  8.57471257e-03 -1.42510049e-02  3.18504535e-02\n",
      "   1.88037147e-03 -3.38938478e-02 -1.57710952e-02 -9.10074121e-03\n",
      "   8.05589577e-02  7.66634003e-02 -1.49575838e-01  3.76557238e-02\n",
      "   2.08776415e-02 -1.50379583e-01 -2.60754560e-01  9.10702589e-01\n",
      "  -1.11773731e-01  9.30838639e-02 -3.81399860e-02 -2.75560164e-02\n",
      "   3.07852527e-02 -5.82915208e-03  1.54357897e-02  1.53321422e-02\n",
      "   2.90075849e-03 -6.31144847e-03 -6.87160918e-03  5.89482371e-03]]\n",
      "[[0.00495452]]\n",
      "[[0.99895652]]\n",
      "[[0.01207416]]\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "# yes\n",
    "test_a = lsa.transform(tf_idf.transform([\"ja\"]))\n",
    "print(test_a)\n",
    "\n",
    "# no\n",
    "test_b = lsa.transform(tf_idf.transform([u'n\\xe4']))\n",
    "\n",
    "# maybe (according to an online dictionary)\n",
    "test_c = lsa.transform(tf_idf.transform([u'kanske']))\n",
    "\n",
    "print(cosine_similarity(test_a, test_b))\n",
    "print(cosine_similarity(test_a, test_c))\n",
    "print(cosine_similarity(test_b, test_c))\n",
    "print(cosine_similarity(test_b, test_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.29011349e-03  2.52727211e-03 -6.34374583e-04  5.55326881e-03\n",
      "  -2.39245892e-04 -1.48188405e-03 -8.16205523e-04  8.42298315e-04\n",
      "  -5.04169499e-03 -2.30353940e-06 -2.36536407e-03 -2.89080734e-03\n",
      "  -1.18691833e-03 -3.04637112e-03  2.09389575e-03 -2.18931325e-04\n",
      "  -3.96857868e-03  1.26094287e-02  6.75262106e-03  3.08682846e-03\n",
      "   3.75317972e-03 -2.40704326e-03 -1.56736199e-03  2.42793815e-03\n",
      "   8.20006228e-04 -1.04234446e-03  8.62725497e-03  2.07789017e-03\n",
      "   2.97357059e-03 -5.56152844e-03 -4.86702876e-03 -2.18053561e-03\n",
      "  -8.87283736e-04 -7.30840720e-04 -1.10542776e-03  2.74007910e-04\n",
      "  -4.39758640e-03 -2.51087103e-04 -1.23747322e-03 -1.52310575e-03\n",
      "  -4.99929199e-03 -7.37718676e-03 -1.84183931e-03  3.44890793e-03\n",
      "   3.63043851e-03 -2.83421161e-03 -2.31805620e-03 -2.04820630e-03\n",
      "   2.62852920e-02 -6.32734432e-03  8.95271579e-04  1.41285686e-03\n",
      "  -4.29371697e-03 -7.01372590e-04 -1.13206355e-03 -6.24236235e-03\n",
      "  -3.60416703e-03  5.59606565e-04 -7.31797130e-03  2.91480704e-03\n",
      "   1.49369202e-03 -4.20431171e-03 -1.59408992e-03  7.22910767e-03\n",
      "  -1.00392711e-02 -1.22113933e-02  1.39145174e-03  9.00902174e-03\n",
      "   2.84723931e-03 -7.53228808e-03 -4.94104253e-03 -3.97148266e-03\n",
      "  -9.34966202e-03  8.57471257e-03 -1.42510049e-02  3.18504535e-02\n",
      "   1.88037147e-03 -3.38938478e-02 -1.57710952e-02 -9.10074121e-03\n",
      "   8.05589577e-02  7.66634003e-02 -1.49575838e-01  3.76557238e-02\n",
      "   2.08776415e-02 -1.50379583e-01 -2.60754560e-01  9.10702589e-01\n",
      "  -1.11773731e-01  9.30838639e-02 -3.81399860e-02 -2.75560164e-02\n",
      "   3.07852527e-02 -5.82915208e-03  1.54357897e-02  1.53321422e-02\n",
      "   2.90075849e-03 -6.31144847e-03 -6.87160918e-03  5.89482371e-03]\n",
      " [ 1.29011349e-03  2.52727211e-03 -6.34374583e-04  5.55326881e-03\n",
      "  -2.39245892e-04 -1.48188405e-03 -8.16205523e-04  8.42298315e-04\n",
      "  -5.04169499e-03 -2.30353940e-06 -2.36536407e-03 -2.89080734e-03\n",
      "  -1.18691833e-03 -3.04637112e-03  2.09389575e-03 -2.18931325e-04\n",
      "  -3.96857868e-03  1.26094287e-02  6.75262106e-03  3.08682846e-03\n",
      "   3.75317972e-03 -2.40704326e-03 -1.56736199e-03  2.42793815e-03\n",
      "   8.20006228e-04 -1.04234446e-03  8.62725497e-03  2.07789017e-03\n",
      "   2.97357059e-03 -5.56152844e-03 -4.86702876e-03 -2.18053561e-03\n",
      "  -8.87283736e-04 -7.30840720e-04 -1.10542776e-03  2.74007910e-04\n",
      "  -4.39758640e-03 -2.51087103e-04 -1.23747322e-03 -1.52310575e-03\n",
      "  -4.99929199e-03 -7.37718676e-03 -1.84183931e-03  3.44890793e-03\n",
      "   3.63043851e-03 -2.83421161e-03 -2.31805620e-03 -2.04820630e-03\n",
      "   2.62852920e-02 -6.32734432e-03  8.95271579e-04  1.41285686e-03\n",
      "  -4.29371697e-03 -7.01372590e-04 -1.13206355e-03 -6.24236235e-03\n",
      "  -3.60416703e-03  5.59606565e-04 -7.31797130e-03  2.91480704e-03\n",
      "   1.49369202e-03 -4.20431171e-03 -1.59408992e-03  7.22910767e-03\n",
      "  -1.00392711e-02 -1.22113933e-02  1.39145174e-03  9.00902174e-03\n",
      "   2.84723931e-03 -7.53228808e-03 -4.94104253e-03 -3.97148266e-03\n",
      "  -9.34966202e-03  8.57471257e-03 -1.42510049e-02  3.18504535e-02\n",
      "   1.88037147e-03 -3.38938478e-02 -1.57710952e-02 -9.10074121e-03\n",
      "   8.05589577e-02  7.66634003e-02 -1.49575838e-01  3.76557238e-02\n",
      "   2.08776415e-02 -1.50379583e-01 -2.60754560e-01  9.10702589e-01\n",
      "  -1.11773731e-01  9.30838639e-02 -3.81399860e-02 -2.75560164e-02\n",
      "   3.07852527e-02 -5.82915208e-03  1.54357897e-02  1.53321422e-02\n",
      "   2.90075849e-03 -6.31144847e-03 -6.87160918e-03  5.89482371e-03]]\n"
     ]
    }
   ],
   "source": [
    "# yes\n",
    "test_a = lsa.transform(tf_idf.transform([u'ja', u'ja']))\n",
    "print(test_a)\n",
    "\n",
    "# no\n",
    "test_b = lsa.transform(tf_idf.transform([u'n\\xe4', u'n\\xe4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out\n",
    "\n",
    "Now that we have our processor, we can start to take a look at the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = (\"anch\", 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matcher(a_vectors, b_vectors, similarity, minimum_matches):\n",
    "    matches = 0\n",
    "    \n",
    "    for vector_a in a_vectors:\n",
    "        for vector_b in b_vectors:\n",
    "            if cosine_similarity(vector_a.reshape(-1, 1), vector_b.reshape(-1, 1))[0][0] > similarity:\n",
    "                matches += 1\n",
    "                \n",
    "    if matches >= minimum_matches:\n",
    "        return True\n",
    "                \n",
    "    return False\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches_anchor_lsa(it, minimum_matches, match_type, overlap, return_count=True, ids=None):\n",
    "    \"\"\"Returns varation set matches using anchor method\"\"\"\n",
    "\n",
    "    matches = 0\n",
    "    matches_list = []\n",
    "\n",
    "    for count, i in enumerate(it):\n",
    "        utterances = iter(i)\n",
    "        first = next(utterances)\n",
    "        first_vector = lsa.transform(tf_idf.transform(first))\n",
    "        \n",
    "        for utterance in utterances:\n",
    "            utterance_vector = lsa.transform(tf_idf.transform(utterance))\n",
    "            if cosine_similarity_matcher(first_vector, utterance_vector, overlap, args[2]):\n",
    "                matches += 1\n",
    "                if ids:\n",
    "                    matches_list.append((ids[count], i))\n",
    "                else:\n",
    "                    matches_list.append(i)\n",
    "\n",
    "    if return_count:\n",
    "        return matches\n",
    "    else:\n",
    "        return matches_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding variation sets inDATA/Swedish_MINGLE_dataset/plain/1\n",
      "Strict match F-score = \t0.0\n",
      "Fuzzy match F-score = \t0.00585930517244\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Evaluation instance has no attribute 'fuzzy_precision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-425d0dd18491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mvarseta_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_utterances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mfuzzy_precisions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvarseta_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuzzy_precision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mstrict_precisions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvarseta_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrict_precision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mfuzzy_recalls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvarseta_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuzzy_recall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Evaluation instance has no attribute 'fuzzy_precision'"
     ]
    }
   ],
   "source": [
    "fuzzy_precisions, strict_precisions, fuzzy_recalls, strict_recalls,\\\n",
    "            fuzzy_f1s, strict_f1s = [], [], [], [], [], []\n",
    "\n",
    "similarity = 0.7\n",
    "\n",
    "for to_do in to_dos:\n",
    "    print(\"Finding variation sets in\" + to_do[0])\n",
    "    u = utterances.Utterances(to_do[0], to_do[1])\n",
    "    gold_utterances = u._goldutterances\n",
    "\n",
    "    utterances_reformatted = []\n",
    "    ids = []\n",
    "\n",
    "    for utterance in u._utterances:\n",
    "        new_utt = utterance[2].split()\n",
    "        \n",
    "        # lowered again\n",
    "        new_utt = [i.lower() for i in new_utt]\n",
    "        utterances_reformatted.append(new_utt)\n",
    "        ids.append((utterance[0], utterance[1]))\n",
    "\n",
    "    utt_iter = vat.window(utterances_reformatted, args[2])\n",
    "    id_iter = vat.window(ids, args[2])\n",
    "    ids = [i for i in id_iter]\n",
    "    ids_and_matches = matches_anchor_lsa(utt_iter, args[2], None, similarity, return_count=False, ids=ids)\n",
    "    combined = vat.convert_varseta_format(ids_and_matches)\n",
    "\n",
    "    varseta_eval = evaluation.Evaluation(combined, gold_utterances)\n",
    "\n",
    "    fuzzy_precisions.append(varseta_eval.fuzzy_precision)\n",
    "    strict_precisions.append(varseta_eval.strict_precision)\n",
    "    fuzzy_recalls.append(varseta_eval.fuzzy_recall)\n",
    "    strict_recalls.append(varseta_eval.strict_recall)\n",
    "    fuzzy_f1s.append(varseta_eval.fuzzy_f1)\n",
    "    strict_f1s.append(varseta_eval.strict_f1)\n",
    "\n",
    "    print('\\tFuzzy Precision: {:0.2f}'.format(varseta_eval.fuzzy_precision))\n",
    "    print('\\tFuzzy Recall: {:0.2f}'.format(varseta_eval.fuzzy_recall))\n",
    "    print('\\tFuzzy F1: {:0.2f}'.format(varseta_eval.fuzzy_f1))\n",
    "    print('')\n",
    "    print('\\tStrict Precision: {:0.2f}'.format(varseta_eval.strict_precision))\n",
    "    print('\\tStrict Recall: {:0.2f}'.format(varseta_eval.strict_recall))\n",
    "    print('\\tStrict F1: {:0.2f}'.format(varseta_eval.strict_f1))\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
